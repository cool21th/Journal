### [Some useful NLP resources](https://www.kaggle.com/c/google-quest-challenge/discussion/127756#729640)


Since I am new to the NLP deep learning world in general, and the transformers one more specifically, I thought it might be interesting to share some resources that I am using:

Obviously, the transformers library.
First place solution to a very recent Q&A NLP competition.
A great kernel to get started with transformers and fastai (what a combo).
Squad 2.0 dataset that could be used to fine-tune pre-trained models.
Since this is a code competition without internet access, this kernel is great to get the transformers library offline. Basically, it installs the library + necessary dependency using code datasets.
Not an NLP link per say but useful if you are new to the code competition format. [EDIT] No need to install, transformers comes in the docker image now (thanks to @adityaecdrid for pointing this out).
I like this illustrated blog post that explains the transformers concept.
A paper on how to fine-tune BERT for classification: "How to Fine-Tune BERT for Text Classification". Thanks @maroberti for the suggestion!
A blog post + video on how to fine-tune BERT: https://mccormickml.com/2019/07/22/BERT-fine-tuning/
If you have other suggestions, I am all ears. Thanks in advance!
